Below is an **alternative approach** that relies **entirely on the LLM** to handle the interactive conversation. We remove the hardcoded `interactive_prompt` logic and instead give the LLM **one comprehensive prompt** (`initial_prompt`) that:

1. **Explains how to respond in JSON**  
2. **Instructs the LLM to ask clarifying questions** if parameters are missing  
3. **Generates either**:
   - A **tool call** when enough information is present  
   - A **follow-up question** (still in JSON) when information is missing

---

# **How This Approach Works**

1. The user asks a question.
2. The LLM reads the conversation + `initial_prompt`.
3. If the user’s question has **all needed parameters** (e.g., `formulary_name`, `year`, `brand_name`), the LLM outputs a JSON with `"tool_calls"`.
4. If **any parameter is missing**, the LLM outputs **a JSON** that indicates a follow-up question, for example:
   ```json
   {
     "follow_up": "Could you please specify the formulary year?"
   }
   ```
5. The application sees `"follow_up"` in the JSON and simply returns that text to the user.
6. Once the user answers, the LLM has the new context. If it’s now sufficient, it outputs `"tool_calls"`.

---

## **1) `prompts.yaml`**

```yaml
prompts:
  initial_prompt: |
    You are an AI assistant that helps users retrieve formulary details, drug placements, UM rules, and WAC.
    **Always** return your output as valid JSON (with double quotes).
    If a user question is missing parameters (like formulary_name, year, or brand_name), ask for them in a follow-up question.
    - Example follow-up JSON:
      {
        "follow_up": "Could you specify the formulary year?"
      }
    If the user’s question has all the necessary parameters, return a JSON with "tool_calls".
    - Example tool_calls JSON:
      {
        "tool_calls": [
          {
            "tool": "query_drug_list_for_formulary",
            "parameters": {
              "formulary_name": "Express Scripts",
              "year": "2023"
            }
          }
        ]
      }
    Do NOT provide plain text outside of JSON. All responses must be in JSON.
```

Explanation:
- We instruct the LLM to **always respond in JSON**.  
- If something’s missing, the LLM must produce a **`follow_up`** field with a clarifying question.  
- If everything is present, the LLM produces **`tool_calls`**.

---

## **2) `ai_gateway.py`**

Below is an example that **relies on the LLM** to decide whether to ask follow-up or produce tool calls. We remove references to an external `interactive_prompt` key, since the LLM itself will generate the follow-up question.

```python
import os
import json
import logging
from openai import AsyncAzureOpenAI
from prompts import prompts

logger = logging.getLogger(__name__)

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")

azure_openai = AsyncAzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version="2023-05-15",
    azure_endpoint=AZURE_OPENAI_ENDPOINT
)

async def process_query(user_query, history):
    """
    Calls Azure OpenAI to parse user query.
    LLM will decide if it has enough info to produce tool_calls 
    or if it needs to ask follow-up questions.
    """
    messages = [
        {"role": "system", "content": prompts["initial_prompt"]}
    ] + history + [{"role": "user", "content": user_query}]
    
    try:
        response = await azure_openai.chat.completions.create(
            model="ai-coe-gpt4-8k-analyze",
            messages=messages,
            temperature=0.7
        )
        
        ai_response = response.choices[0].message.content
        logger.info(f"Raw LLM response: {ai_response}")

        # Attempt to parse as JSON
        try:
            parsed_response = json.loads(ai_response)
        except json.JSONDecodeError:
            # If LLM fails to produce valid JSON, we can handle it gracefully
            return {
                "error": "The AI did not return valid JSON. Please try rephrasing your request."
            }

        return parsed_response

    except Exception as e:
        logger.error(f"Error processing query: {str(e)}")
        return {"error": "AI processing failed."}


async def format_response_with_llm(results, history):
    """Use the LLM to format search results into a user-friendly response."""
    # If we got an error from OpenSearch, just return that
    if isinstance(results, dict) and "error" in results:
        return results["error"]

    # If we got an array with an "error" in the first element
    if isinstance(results, list) and results and "error" in results[0]:
        return results[0]["error"]

    # Convert results to string for the LLM to reformat
    results_str = json.dumps(results, indent=2)

    messages = [
        {"role": "system", "content": "Reformat the search results into a clear, concise response. Use bullet points or short paragraphs."}
    ] + history + [{"role": "assistant", "content": results_str}]

    try:
        response = await azure_openai.chat.completions.create(
            model="ai-coe-gpt4-8k-analyze",
            messages=messages,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error formatting response: {str(e)}")
        return results_str
```

### Key Changes:
- We **removed** references to `interactive_prompt`.  
- The LLM itself is told (via `initial_prompt`) to produce `"follow_up"` or `"tool_calls"`.

---

## **3) `routes.py`**

The router checks whether the LLM wants to **ask a follow-up** or **execute a tool call**:

```python
from fastapi import APIRouter
from services.ai_gateway import process_query, format_response_with_llm
from services.opensearch import query_opensearch
import logging

router = APIRouter()

conversation_history = {}  # Keep conversation by conversation_id

@router.post("/chat/{conversation_id}")
async def ask_bot(conversation_id: str, request: dict):
    """Process user query, handle follow-ups, or run tool calls."""
    
    user_query = request.get("query", "")
    logging.info(f"User query: {user_query}")

    # Retrieve or init conversation history
    if conversation_id not in conversation_history:
        conversation_history[conversation_id] = []
    history = conversation_history[conversation_id]

    # Add user message to history
    history.append({"role": "user", "content": user_query})

    # Send to LLM
    ai_response = await process_query(user_query, history)

    # Add LLM response to history
    history.append({"role": "assistant", "content": str(ai_response)})

    # If the LLM returned an error or is not dict, just respond
    if not isinstance(ai_response, dict):
        return {"response": ai_response}

    # If LLM says it needs more info
    if "follow_up" in ai_response:
        return {"response": ai_response["follow_up"]}

    # If we have tool calls, do them
    if "tool_calls" in ai_response:
        results = await query_opensearch(ai_response["tool_calls"], history)
        formatted_results = await format_response_with_llm(results, history)
        return {"response": formatted_results}

    # Otherwise just return
    return {"response": ai_response}
```

### Key Points:
- We no longer use a separate `interactive_prompt`.  
- We rely on `"follow_up"` being generated by the LLM if something is missing.

---

## **4) `opensearch.py`**

No big change needed here; it just executes queries if `"tool_calls"` are present.

```python
import os
import requests
import logging

logger = logging.getLogger(__name__)

OPENSEARCH_URL = os.getenv("OPENSEARCH_URL", "http://localhost:9200/fps_um/_search")

async def query_opensearch(tool_calls, conversation_history):
    """Executes queries against OpenSearch based on tool calls."""
    
    results = []
    for tool_call in tool_calls:
        tool = tool_call["tool"]
        params = tool_call["parameters"]
        
        query_conditions = []
        
        if tool == "query_drug_list_for_formulary":
            # e.g., user wants all drugs in a given formulary for a given year
            if not params.get("formulary_name") or not params.get("year"):
                results.append({"error": "Missing formulary_name or year."})
                continue

            query_conditions.append({"match_phrase": {"formulary_name.keyword": params["formulary_name"]}})
            query_conditions.append({"match_phrase": {"invoice_yr.keyword": str(params["year"])}})

        elif tool == "query_fps_placement":
            # etc...
            pass
        # ... handle other tools similarly ...

        # Build the final query
        query = {"size": 1000, "query": {"bool": {"must": query_conditions}}}
        logger.info(f"Querying OpenSearch with: {query}")
        
        # Send request
        response = requests.post(OPENSEARCH_URL, json=query)
        if response.status_code == 200:
            data = response.json()
            hits = data.get("hits", {}).get("hits", [])
            # Extract relevant fields from each doc
            docs = [hit["_source"] for hit in hits]
            results.append(docs)
        else:
            results.append({"error": "Failed to fetch data from OpenSearch."})

    return results
```

---

## **Example Interaction**

1. **User**: "What are the drugs under Express Scripts?"  
   - LLM sees `formulary_name="Express Scripts"` but **no year**.  
   - LLM returns:
     ```json
     { "follow_up": "For which year do you need the drug list?" }
     ```
2. **User**: "For 2023."  
   - Now the user has provided the missing year.  
   - LLM sees `formulary_name="Express Scripts"` and `year=2023`, so it returns:
     ```json
     {
       "tool_calls": [
         {
           "tool": "query_drug_list_for_formulary",
           "parameters": {
             "formulary_name": "Express Scripts",
             "year": 2023
           }
         }
       ]
     }
     ```
3. **Bot** runs `query_opensearch` and returns the results.

---

# **✅ Key Takeaways**
- **All interactive logic** is controlled by the LLM using a single `initial_prompt`.  
- The LLM can produce:
  1. **`"follow_up"`** if missing data  
  2. **`"tool_calls"`** if ready to query  
- We **no longer** have a separate `interactive_prompt`; it’s all within `initial_prompt`.  
- The code only checks for `"follow_up"` or `"tool_calls"` in the LLM’s JSON.

This approach **maximizes the LLM’s flexibility** to handle user queries and ask clarifying questions **in a purely conversational style**.

To achieve this, we need to modify our approach so that when the bot **detects missing parameters**, it fetches **valid options from OpenSearch** before returning a `"follow_up"` response.

### **ðŸ”¹ Steps to Achieve This**
1. **If the user query lacks a required parameter**, the LLM generates a `"follow_up"` response.
2. **Before returning the `"follow_up"` question**, we query OpenSearch to get valid options.
3. **The bot includes these options in the `"follow_up"` response** instead of hardcoding them.

---

# **âœ… Updated `ai_gateway.py`**
This modification ensures that the bot queries OpenSearch for valid options before returning a follow-up response.

```python
import os
import json
import logging
from openai import AsyncAzureOpenAI
from prompts import prompts
from services.opensearch import query_valid_options

logger = logging.getLogger(__name__)

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")

azure_openai = AsyncAzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version="2023-05-15",
    azure_endpoint=AZURE_OPENAI_ENDPOINT
)

async def process_query(user_query, history):
    """
    Calls Azure OpenAI to determine the next action:
    - If missing parameters, queries OpenSearch for valid options.
    - If an NLP response is needed, returns a structured message.
    - If ready for OpenSearch, returns tool_calls.
    """
    messages = [
        {"role": "system", "content": prompts["initial_prompt"]}
    ] + history + [{"role": "user", "content": user_query}]
    
    try:
        response = await azure_openai.chat.completions.create(
            model="ai-coe-gpt4-8k-analyze",
            messages=messages,
            temperature=0.7
        )
        
        ai_response = response.choices[0].message.content
        logger.info(f"Raw LLM response: {ai_response}")

        # Convert LLM response to Python dict
        parsed_response = json.loads(ai_response)

        # Validate response_type
        if "response_type" not in parsed_response:
            return {
                "response_type": "message",
                "message": {
                    "text": "Invalid AI response format.",
                    "context": "error"
                }
            }

        # Handle follow-up cases by fetching valid options from OpenSearch
        if parsed_response["response_type"] == "follow_up":
            follow_up_question = parsed_response["follow_up"]["question"]

            # Determine what data needs to be fetched
            if "formulary" in follow_up_question.lower():
                options = await query_valid_options("formulary")
            elif "year" in follow_up_question.lower():
                options = await query_valid_options("year")
            elif "drug" in follow_up_question.lower():
                options = await query_valid_options("drug")
            else:
                options = []

            # Return follow-up with dynamically fetched options
            return {
                "response_type": "follow_up",
                "follow_up": {
                    "question": follow_up_question,
                    "options": options
                }
            }

        return parsed_response  # Returning Python dict (FastAPI auto-converts to JSON)

    except json.JSONDecodeError:
        logger.error("LLM response is not valid JSON.")
        return {
            "response_type": "message",
            "message": {
                "text": "The AI response could not be processed.",
                "context": "error"
            }
        }

    except Exception as e:
        logger.error(f"Error processing query: {str(e)}")
        return {
            "response_type": "message",
            "message": {
                "text": "AI processing failed.",
                "context": "error"
            }
        }
```

---

# **âœ… New Function in `opensearch.py`**
This function **fetches valid options** from OpenSearch **before asking a follow-up question**.

```python
import os
import requests
import logging

logger = logging.getLogger(__name__)

OPENSEARCH_URL = os.getenv("OPENSEARCH_URL", "http://localhost:9200/fps_um/_search")

async def query_valid_options(option_type):
    """
    Fetches valid options (formulary names, years, or drugs) from OpenSearch.
    """
    query_conditions = []

    if option_type == "formulary":
        query = {
            "size": 1000,
            "_source": ["formulary_name"],
            "query": {"match_all": {}}
        }
    elif option_type == "year":
        query = {
            "size": 1000,
            "_source": ["invoice_yr"],
            "query": {"match_all": {}}
        }
    elif option_type == "drug":
        query = {
            "size": 1000,
            "_source": ["brand_name"],
            "query": {"match_all": {}}
        }
    else:
        return []

    response = requests.post(OPENSEARCH_URL, json=query)
    
    if response.status_code == 200:
        data = response.json()
        hits = data.get("hits", {}).get("hits", [])
        options = list(set(hit["_source"][option_type] for hit in hits if option_type in hit["_source"]))
        return sorted(options)  # Returning sorted list of unique options
    else:
        return []
```

---

# **âœ… Updated `routes.py`**
This ensures that if `"follow_up"` is returned, the bot **presents dynamic options**.

```python
from fastapi import APIRouter
from services.ai_gateway import process_query, format_response_with_llm
from services.opensearch import query_opensearch
import logging

router = APIRouter()
conversation_history = {}

@router.post("/chat/{conversation_id}")
async def ask_bot(conversation_id: str, request: dict):
    """Process user query, handle follow-ups, or run tool calls."""
    
    user_query = request.get("query", "")
    logging.info(f"User query: {user_query}")

    # Retrieve or init conversation history
    if conversation_id not in conversation_history:
        conversation_history[conversation_id] = []
    history = conversation_history[conversation_id]

    # Add user message to history
    history.append({"role": "user", "content": user_query})

    # Send to LLM
    ai_response = await process_query(user_query, history)

    # Add LLM response to history
    history.append({"role": "assistant", "content": str(ai_response)})

    # Handle response types
    response_type = ai_response.get("response_type")

    if response_type == "follow_up":
        return {
            "response_type": "follow_up",
            "follow_up": ai_response["follow_up"]
        }

    if response_type == "message":
        return {
            "response_type": "message",
            "message": ai_response["message"]
        }

    if response_type == "tool_calls":
        results = await query_opensearch(ai_response["tool_calls"], history)
        formatted_results = await format_response_with_llm(results, history)
        return {
            "response_type": "message",
            "message": {
                "text": formatted_results,
                "context": "info"
            }
        }

    return {
        "response_type": "message",
        "message": {
            "text": "Unexpected response type.",
            "context": "error"
        }
    }
```

---

# **âœ… Example Bot Behavior (Dynamic Follow-Ups)**

### **Case 1: Missing Year (Follow-Up with Valid Options)**
ðŸ‘¤ **User**: *"What are the drugs under Express Scripts?"*  
ðŸ¤– **Bot Response (JSON)** (Fetched available years from OpenSearch)
```json
{
  "response_type": "follow_up",
  "follow_up": {
    "question": "For which year do you need the drug list?",
    "options": ["2021", "2022", "2023"]
  }
}
```

---

### **Case 2: Missing Formulary (Follow-Up with Options)**
ðŸ‘¤ **User**: *"What is the placement of Lipitor?"*  
ðŸ¤– **Bot Response (JSON)** (Fetched valid formularies from OpenSearch)
```json
{
  "response_type": "follow_up",
  "follow_up": {
    "question": "Which formulary are you referring to?",
    "options": ["Express Scripts", "BlueCross", "Aetna"]
  }
}
```

---

### **ðŸš€ Summary of Fixes**
âœ” **The bot now queries OpenSearch for valid options before asking follow-ups.**  
âœ” **The follow-up response contains real options, not hardcoded ones.**  
âœ” **Ensures consistency in structured responses.**  
âœ” **Dynamic interactionâ€”bot intelligently asks for missing details without guessing.**  

ðŸš€ **Now your bot can fetch real options dynamically before asking follow-ups!** Let me know if you need refinements! ðŸŽ¯

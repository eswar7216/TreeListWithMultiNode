You're absolutely right! We first need to **determine the user's intent (context)** before deciding whether to:
1. **Perform an OpenSearch lookup** (for `"did_you_mean"` or `"follow_up"`).
2. **Return a direct NLP response** if it's not a formulary-related question.
3. **Execute a tool call** if all parameters are available.

---

## **üîπ Updated Approach**
### **Step 1: Determine User Intent**
- **Use LLM to categorize the query**:
  - Is it an **NLP message** (e.g., "How does formulary work?")?
  - Is it a **formulary-related search** (e.g., "What is the WAC for Lipitor?")?

### **Step 2: If It‚Äôs a Formulary Question, Perform OpenSearch Check**
- If an **exact match exists**, proceed with the query.
- If **no exact match**, check for similar matches and return `"did_you_mean"`.

### **Step 3: If Data is Missing, Ask for Follow-Up**
- If **a required parameter is missing** (e.g., **year is missing**), return a `"follow_up"` response.

---

## **‚úÖ Updated `ai_gateway.py`**
This update **first derives the context** before performing OpenSearch operations.

```python
import os
import json
import logging
from openai import AsyncAzureOpenAI
from prompts import prompts
from services.opensearch import query_valid_options, query_did_you_mean

logger = logging.getLogger(__name__)

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")

azure_openai = AsyncAzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version="2023-05-15",
    azure_endpoint=AZURE_OPENAI_ENDPOINT
)

async def derive_context(user_query, history):
    """
    Uses LLM to determine if the query is:
    1. NLP (general knowledge)
    2. A formulary-related search
    """
    messages = [
        {"role": "system", "content": prompts["context_prompt"]}
    ] + history + [{"role": "user", "content": user_query}]
    
    try:
        response = await azure_openai.chat.completions.create(
            model="ai-coe-gpt4-8k-analyze",
            messages=messages,
            temperature=0.7
        )

        ai_response = response.choices[0].message.content
        logger.info(f"Derived Context: {ai_response}")

        parsed_response = json.loads(ai_response)

        if "context" not in parsed_response:
            return {"context": "unknown"}

        return parsed_response

    except json.JSONDecodeError:
        return {"context": "unknown"}

    except Exception as e:
        logger.error(f"Context derivation failed: {str(e)}")
        return {"context": "unknown"}

async def process_query(user_query, history):
    """
    Determines the context of the query before deciding whether to:
    - Process an NLP question
    - Query OpenSearch for exact/fuzzy matches
    - Perform a follow-up if necessary
    """
    
    # Step 1: Determine query intent
    context_result = await derive_context(user_query, history)
    context = context_result.get("context")

    if context == "nlp":
        return {
            "response_type": "message",
            "message": {
                "text": "I'm happy to answer general formulary questions!",
                "context": "info"
            }
        }

    if context == "formulary_search":
        entity = context_result.get("entity", "")

        # Step 2: Check OpenSearch for exact match
        exact_match = await query_valid_options(entity)
        
        if not exact_match:  # No exact match, try fuzzy search
            fuzzy_matches = await query_did_you_mean(entity)
            
            if fuzzy_matches:
                return {
                    "response_type": "did_you_mean",
                    "did_you_mean": {
                        "question": "Did you mean one of these?",
                        "options": fuzzy_matches
                    }
                }

        # Step 3: If an exact match exists, check if follow-up is needed
        missing_parameters = context_result.get("missing_parameters", [])

        if missing_parameters:
            options = await query_valid_options(missing_parameters[0])
            return {
                "response_type": "follow_up",
                "follow_up": {
                    "question": f"Could you provide {missing_parameters[0]}?",
                    "options": options
                }
            }

        # Step 4: If everything is provided, generate tool_calls
        return {
            "response_type": "tool_calls",
            "tool_calls": [
                {
                    "tool": context_result["tool"],
                    "parameters": context_result["parameters"]
                }
            ]
        }

    return {
        "response_type": "message",
        "message": {
            "text": "I couldn't determine your request. Please clarify.",
            "context": "error"
        }
    }
```

---

## **‚úÖ Updated `prompts.yaml`**
We **add a new `context_prompt`** so the LLM can classify queries.

```yaml
prompts:
  context_prompt: |
    You are an AI assistant that determines the intent of a user query.
    
    **Your response must be in JSON format and include a "context" key.**
    
    - If the user is asking a general NLP question (not related to OpenSearch), respond:
      {
        "context": "nlp"
      }

    - If the user is asking about a formulary (e.g., "What is the WAC for Lipitor?"), respond:
      {
        "context": "formulary_search",
        "entity": "<extracted entity>",
        "tool": "query_total_wac",
        "parameters": {
          "formulary_name": "<extracted formulary>",
          "year": "<extracted year>",
          "brand_name": "<extracted brand>"
        },
        "missing_parameters": ["year"]  // If missing details
      }

    - If the query is unclear, respond:
      {
        "context": "unknown"
      }

    **Always return JSON. Never return plain text.**
```

---

## **3Ô∏è‚É£ Example Scenarios**

### **‚úÖ Scenario 1: NLP Question**
üë§ **User**: *"How does formulary placement work?"*  
üîç **LLM determines it's an NLP query**  
ü§ñ **Bot Response (JSON)**:
```json
{
  "response_type": "message",
  "message": {
    "text": "I'm happy to answer general formulary questions!",
    "context": "info"
  }
}
```

---

### **‚úÖ Scenario 2: Did You Mean?**
üë§ **User**: *"What are the drugs under Expres Scipts?"*  
üîç **LLM detects it's a formulary search**  
üîç **OpenSearch finds no exact match, but fuzzy search suggests options**  
ü§ñ **Bot Response (JSON)**:
```json
{
  "response_type": "did_you_mean",
  "did_you_mean": {
    "question": "Did you mean one of these?",
    "options": ["Express Scripts", "BlueCross", "Aetna"]
  }
}
```

---

### **‚úÖ Scenario 3: Follow-Up**
üë§ **User**: *"What is the WAC for Lipitor?"*  
üîç **LLM detects a formulary search**  
üîç **OpenSearch confirms a match, but year is missing**  
ü§ñ **Bot Response (JSON)**:
```json
{
  "response_type": "follow_up",
  "follow_up": {
    "question": "Could you provide the year?",
    "options": ["2021", "2022", "2023"]
  }
}
```

---

### **‚úÖ Scenario 4: Full Query**
üë§ **User**: *"What is the WAC for Lipitor under Express Scripts in 2023?"*  
üîç **LLM confirms all required parameters**  
üîç **OpenSearch finds an exact match**  
ü§ñ **Bot Calls OpenSearch**:
```json
{
  "response_type": "tool_calls",
  "tool_calls": [
    {
      "tool": "query_total_wac",
      "parameters": {
        "formulary_name": "Express Scripts",
        "year": 2023,
        "brand_name": "Lipitor"
      }
    }
  ]
}
```

---

### **üöÄ Summary of Fixes**
‚úî **First derives the query context before using OpenSearch.**  
‚úî **If no match is found, suggests closest matches (`did_you_mean`).**  
‚úî **If a required parameter is missing, asks for it (`follow_up`).**  
‚úî **Only calls OpenSearch when all parameters are present.**  

üöÄ **Now your bot is context-aware, fully interactive, and handles vague queries properly!** Let me know if you need more refinements! üéØ

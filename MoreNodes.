from fastapi import APIRouter
from services.ai_gateway import process_query, format_response_with_llm
from services.opensearch import query_opensearch
import logging

router = APIRouter()
conversation_history = {}

@router.post("/chat/{conversation_id}")
async def ask_bot(conversation_id: str, request: dict):
    """Process user query, handle follow-ups, or run tool calls."""
    
    user_query = request.get("query", "")
    logging.info(f"User query: {user_query}")

    # Retrieve or init conversation history
    if conversation_id not in conversation_history:
        conversation_history[conversation_id] = []
    history = conversation_history[conversation_id]

    # Add user message to history
    history.append({"role": "user", "content": user_query})

    # Send to LLM
    ai_response = await process_query(user_query, history)

    # Add LLM response to history
    history.append({"role": "assistant", "content": str(ai_response)})

    # Handle response types
    response_type = ai_response.get("response_type")

    if response_type == "follow_up":
        return {
            "response_type": "follow_up",
            "follow_up": ai_response["follow_up"]
        }

    if response_type == "message":
        return {
            "response_type": "message",
            "message": ai_response["message"]
        }

    if response_type == "tool_calls":
        results = await query_opensearch(ai_response["tool_calls"], history)
        formatted_results = await format_response_with_llm(results, history)
        return {
            "response_type": "message",
            "message": {
                "text": formatted_results,
                "context": "info"
            }
        }

    return {
        "response_type": "message",
        "message": {
            "text": "Unexpected response type.",
            "context": "error"
        }
    }
